{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "36bddf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import html\n",
    "import re\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c921dd12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 100 posts so far...\n",
      "Fetched 200 posts so far...\n",
      "Reached the last available post.\n",
      "Saved 238 posts to 'file'\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.reddit.com/r/AmItheAsshole/search.json\"\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36\"}\n",
    "\n",
    "def clean_dict(d):\n",
    "    if isinstance(d, dict):\n",
    "        return {key: clean_dict(value) for key, value in d.items()}\n",
    "    elif isinstance(d, list):\n",
    "        return [clean_dict(item) for item in d]\n",
    "    elif isinstance(d, str):\n",
    "        return html.unescape(d).replace(\"\\n\\n\", \" \").replace(\"\\n\", \"\").replace(\"\\\"\", \"\").replace(\"\\\\\", \"\").replace(\"&#x200B;\", \"\").replace(\"\\\\u2019\", \"'\")\n",
    "    else:\n",
    "        return d\n",
    "\n",
    "def fetch_reddit_posts(max_posts=1000):\n",
    "    \n",
    "    posts = []\n",
    "    after = None  # Pagination token\n",
    "\n",
    "    while len(posts) < max_posts:\n",
    "        params = {\n",
    "            \"q\": 'flair:\"Not the A-hole\"',  # Search for flair\n",
    "            \"restrict_sr\": 1,        # Restrict search to this subreddit\n",
    "            \"sort\": \"top\",           # Sort by\n",
    "            \"limit\": 100,            # Fetch 100 posts per request (max allowed)\n",
    "            \"after\": after,          # Pagination token\n",
    "            \"t\" : \"year\"\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, headers=headers, params=params)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error {response.status_code}: {response.text}\")\n",
    "            break\n",
    "\n",
    "        data = response.json()\n",
    "        children = data[\"data\"][\"children\"]\n",
    "\n",
    "        if not children:\n",
    "            print(\"No more posts found.\")\n",
    "            break\n",
    "\n",
    "        # Extract filtered fields\n",
    "        for post in children:\n",
    "            post_data = post[\"data\"]\n",
    "            filtered_post = {\n",
    "                \"selftext\": post_data.get(\"selftext\"),\n",
    "                \"gilded\": post_data.get(\"gilded\"),\n",
    "                \"title\": post_data.get(\"title\"),\n",
    "                \"upvote_ratio\": post_data.get(\"upvote_ratio\"),\n",
    "                \"ups\": post_data.get(\"ups\"),\n",
    "                \"link_flair_text\" : post_data.get(\"link_flair_text\"),\n",
    "                \"created\" : post_data.get(\"created\"),\n",
    "                \"num_comments\" : post_data.get(\"num_comments\"),\n",
    "                \"url\" : post_data.get(\"url\"),\n",
    "                \"num_crossposts\" : post_data.get(\"num_crossposts\")\n",
    "            }\n",
    "            cleaned_post = clean_dict(filtered_post)\n",
    "            posts.append(cleaned_post)\n",
    "\n",
    "        # Get `after` token for pagination\n",
    "        after = data[\"data\"].get(\"after\")\n",
    "        if not after:\n",
    "            print(\"Reached the last available post.\")\n",
    "            break\n",
    "\n",
    "        print(f\"Fetched {len(posts)} posts so far...\")\n",
    "\n",
    "        time.sleep(3.5)\n",
    "\n",
    "    return posts[:max_posts]\n",
    "\n",
    "posts = fetch_reddit_posts(max_posts=1000)\n",
    "\n",
    "with open(\"nta_topyear_2025-2-7.json\", \"a\", encoding=\"utf-8\") as file:\n",
    "    json.dump(posts, file, indent=4)\n",
    "\n",
    "print(f\"Saved {len(posts)} posts to 'file'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d3ae009c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"IMDB Dataset.csv\")\n",
    "\n",
    "# apply function to convert labels to what fasttext expects: __label__[label]\n",
    "df['sentiment'] = df['sentiment'].apply(lambda x: f\"__label__{x}\")  # Example: \"positive\" -> \"__label__positive\"\n",
    "\n",
    "# reverse order of columns\n",
    "df = df[['sentiment', 'review']]\n",
    "df['review'] = df['review'].str.replace(\"<br /><br />\", \" \").str.replace('\\u200b', '').str.strip()\n",
    "df.to_csv(\"fasttext_train_temp.txt\", index=False, sep=\" \", header=False, escapechar=\" \", quoting=csv.QUOTE_NONE)\n",
    "\n",
    "with open(\"fasttext_train_temp.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "cleaned_lines = []\n",
    "for line in lines:\n",
    "    line = line.replace(\"  \", \" \")\n",
    "    cleaned_lines.append(line)\n",
    "\n",
    "# final fasttext training corpus stored in \"fasttext_train.txt\"\n",
    "with open(\"fasttext_train.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.writelines(cleaned_lines)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
