{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36bddf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import html\n",
    "import re\n",
    "import pandas as pd\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea41fdb",
   "metadata": {},
   "source": [
    "<h2>Scrape Reddit Data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c921dd12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 100 posts so far...\n",
      "Fetched 200 posts so far...\n",
      "Reached the last available post.\n",
      "Saved 238 posts to 'file'\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.reddit.com/r/AmItheAsshole/search.json\"\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36\"}\n",
    "\n",
    "def clean_dict(d):\n",
    "    if isinstance(d, dict):\n",
    "        return {key: clean_dict(value) for key, value in d.items()}\n",
    "    elif isinstance(d, list):\n",
    "        return [clean_dict(item) for item in d]\n",
    "    elif isinstance(d, str):\n",
    "        return html.unescape(d).replace(\"\\n\\n\", \" \").replace(\"\\n\", \"\").replace(\"\\\"\", \"\").replace(\"\\\\\", \"\").replace(\"&#x200B;\", \"\").replace(\"\\\\u2019\", \"'\")\n",
    "    else:\n",
    "        return d\n",
    "\n",
    "def fetch_reddit_posts(max_posts=1000):\n",
    "    \n",
    "    posts = []\n",
    "    after = None  # Pagination token\n",
    "\n",
    "    while len(posts) < max_posts:\n",
    "        params = {\n",
    "            \"q\": 'flair:\"Not the A-hole\"',  # Search for flair\n",
    "            \"restrict_sr\": 1,        # Restrict search to this subreddit\n",
    "            \"sort\": \"top\",           # Sort by\n",
    "            \"limit\": 100,            # Fetch 100 posts per request (max allowed)\n",
    "            \"after\": after,          # Pagination token\n",
    "            \"t\" : \"year\"\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, headers=headers, params=params)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error {response.status_code}: {response.text}\")\n",
    "            break\n",
    "\n",
    "        data = response.json()\n",
    "        children = data[\"data\"][\"children\"]\n",
    "\n",
    "        if not children:\n",
    "            print(\"No more posts found.\")\n",
    "            break\n",
    "\n",
    "        # Extract filtered fields\n",
    "        for post in children:\n",
    "            post_data = post[\"data\"]\n",
    "            filtered_post = {\n",
    "                \"selftext\": post_data.get(\"selftext\"),\n",
    "                \"gilded\": post_data.get(\"gilded\"),\n",
    "                \"title\": post_data.get(\"title\"),\n",
    "                \"upvote_ratio\": post_data.get(\"upvote_ratio\"),\n",
    "                \"ups\": post_data.get(\"ups\"),\n",
    "                \"link_flair_text\" : post_data.get(\"link_flair_text\"),\n",
    "                \"created\" : post_data.get(\"created\"),\n",
    "                \"num_comments\" : post_data.get(\"num_comments\"),\n",
    "                \"url\" : post_data.get(\"url\"),\n",
    "                \"num_crossposts\" : post_data.get(\"num_crossposts\")\n",
    "            }\n",
    "            cleaned_post = clean_dict(filtered_post)\n",
    "            posts.append(cleaned_post)\n",
    "\n",
    "        # Get after token for pagination\n",
    "        after = data[\"data\"].get(\"after\")\n",
    "        if not after:\n",
    "            print(\"Reached the last available post.\")\n",
    "            break\n",
    "\n",
    "        print(f\"Fetched {len(posts)} posts so far...\")\n",
    "\n",
    "        time.sleep(3.5)\n",
    "\n",
    "    return posts[:max_posts]\n",
    "\n",
    "posts = fetch_reddit_posts(max_posts=1000)\n",
    "\n",
    "with open(\"reddit_data/nta_topyear_2025-2-7.json\", \"a\", encoding=\"utf-8\") as file:\n",
    "    json.dump(posts, file, indent=4)\n",
    "\n",
    "print(f\"Saved {len(posts)} posts to file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bc4f35",
   "metadata": {},
   "source": [
    "<h2>Format Corpus for IMDB Data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d3ae009c",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_path = \"C:/Users/maddo/CS770_data/project_data/imdb_data\"\n",
    "\n",
    "# read raw data csv file\n",
    "df = pd.read_csv(imdb_path + \"/IMDB Dataset.csv\")\n",
    "\n",
    "# apply function to convert labels to what fasttext expects: __label__[label]\n",
    "df['sentiment'] = df['sentiment'].apply(lambda x: f\"__label__{x}\")  # Example: \"positive\" -> \"__label__positive\"\n",
    "\n",
    "# reverse order of columns\n",
    "df = df[['sentiment', 'review']]\n",
    "\n",
    "# remove spare html left in review contents\n",
    "df['review'] = df['review'].str.replace(\"<br /><br />\", \" \").str.replace('\\u200b', '').str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c33b55-5ede-4c9a-9f2a-354ce788cd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save partially cleaned data into text file\n",
    "df.to_csv(imdb_path + \"/fasttext_imdb_corpus_temp.txt\", index=False, sep=\" \", header=False, escapechar=\" \", quoting=csv.QUOTE_NONE)\n",
    "\n",
    "with open(imdb_path + \"/fasttext_imdb_corpus_temp.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "cleaned_lines = []\n",
    "for line in lines:\n",
    "    line = line.replace(\"  \", \" \") # fix double spacing in temp file\n",
    "    cleaned_lines.append(line)\n",
    "\n",
    "# fully-cleaned fasttext training corpus stored in \"fasttext_imdb_corpus.txt\"\n",
    "with open(imdb_path + \"/fasttext_imdb_corpus.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.writelines(cleaned_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df32751-1316-43af-84ea-07499423c73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['review']\n",
    "y = df['sentiment']\n",
    "\n",
    "# divide dataset into train, test, and validation sets (70, 20, 10 split)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "X_test, X_tune, y_test, y_tune = train_test_split(X_temp, y_temp, test_size=0.333, random_state=42, stratify=y_temp)\n",
    "\n",
    "# create list of dataframes to iterate through later\n",
    "sets = []\n",
    "setnames = ['train', 'test', 'tune']\n",
    "\n",
    "train = y_train.to_frame().join(X_train) # create training set\n",
    "sets.append(train)\n",
    "\n",
    "test = y_test.to_frame().join(X_test) # create testing set\n",
    "sets.append(test)\n",
    "\n",
    "tune = y_tune.to_frame().join(X_tune) # create validation set\n",
    "sets.append(tune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a82f4b-7de3-4b46-ae36-5180c1cda413",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for set in sets:\n",
    "    # write set contents to file\n",
    "    set.to_csv(f\"{imdb_path}/fasttext_{setnames[i]}_temp.txt\", index=False, sep=\" \", header=False, escapechar=\" \", quoting=csv.QUOTE_NONE)\n",
    "    \n",
    "    with open(f\"{imdb_path}/fasttext_{setnames[i]}_temp.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines() # read set contents from file\n",
    "    \n",
    "    cleaned_lines = []\n",
    "    for line in lines:\n",
    "        line = line.replace(\"  \", \" \") # clean space padding\n",
    "        cleaned_lines.append(line)\n",
    "\n",
    "    # write fully cleaned set to file for fasttext to use\n",
    "    if setnames[i] == 'tune':\n",
    "        with open(f\"{imdb_path}/fasttext_{setnames[i]}.valid\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.writelines(cleaned_lines)\n",
    "    else:\n",
    "        with open(f\"{imdb_path}/fasttext_{setnames[i]}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.writelines(cleaned_lines)\n",
    "    \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06116cc-38c3-47bf-b575-cd955c784f64",
   "metadata": {},
   "source": [
    "<h2>Format Corpus for Reddit Data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb217ea6-e61c-4b2b-a3a3-32b5f8b22e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_path = 'reddit_data'\n",
    "\n",
    "# load json file containing reddit post data\n",
    "with open('reddit_data/aita_data.json') as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "# get posts as a dataframe\n",
    "data = pd.json_normalize(raw_data['posts']).drop_duplicates()\n",
    "\n",
    "data = data[['link_flair_text', 'selftext']]\n",
    "data = data.rename(columns={'link_flair_text': 'verdict', 'selftext': 'content'})\n",
    "data['content'] = data['content'].apply(lambda x: x.strip())\n",
    "\n",
    "# convert verdicts into either label negative (nta), or label positive (yta)\n",
    "data['verdict'] = data['verdict'].replace({'Asshole' : '__label__positive', 'Asshole POO Mode' : '__label__positive', 'Not the A-hole' : '__label__negative', 'Not the A-hole POO Mode' : '__label__negative'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85bdf0ff-430e-404d-801c-def8f505ad59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save partially cleaned data into text file\n",
    "data.to_csv(reddit_path + \"/fasttext_reddit_corpus_temp.txt\", index=False, sep=\" \", header=False, escapechar=\" \", quoting=csv.QUOTE_NONE)\n",
    "\n",
    "with open(reddit_path + \"/fasttext_reddit_corpus_temp.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "cleaned_lines = []\n",
    "for line in lines:\n",
    "    line = line.replace(\"  \", \" \") # fix double spacing in temp file\n",
    "    cleaned_lines.append(line)\n",
    "\n",
    "# fully-cleaned fasttext training corpus stored in \"fasttext_yelp_corpus.txt\"\n",
    "with open(reddit_path + \"/fasttext_reddit_corpus.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.writelines(cleaned_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8606bf5a-ba99-461f-bbb2-01df63676ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['content']\n",
    "y = data['verdict']\n",
    "\n",
    "# divide dataset into train, test, and validation sets (70, 20, 10 split)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "X_test, X_tune, y_test, y_tune = train_test_split(X_temp, y_temp, test_size=0.333, random_state=42, stratify=y_temp)\n",
    "\n",
    "# create list of dataframes to iterate through later\n",
    "sets = []\n",
    "setnames = ['train', 'test', 'tune']\n",
    "\n",
    "train = y_train.to_frame().join(X_train) # create training set\n",
    "sets.append(train)\n",
    "\n",
    "test = y_test.to_frame().join(X_test) # create testing set\n",
    "sets.append(test)\n",
    "\n",
    "tune = y_tune.to_frame().join(X_tune) # create validation set\n",
    "sets.append(tune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3924ffdc-edea-4f3d-9c96-89bb664c73d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for set in sets:\n",
    "    # write set contents to file\n",
    "    set.to_csv(f\"{reddit_path}/fasttext_{setnames[i]}_temp.txt\", index=False, sep=\" \", header=False, escapechar=\" \", quoting=csv.QUOTE_NONE)\n",
    "    \n",
    "    with open(f\"{reddit_path}/fasttext_{setnames[i]}_temp.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines() # read set contents from file\n",
    "    \n",
    "    cleaned_lines = []\n",
    "    for line in lines:\n",
    "        line = line.replace(\"  \", \" \") # clean space padding\n",
    "        cleaned_lines.append(line)\n",
    "\n",
    "    # write fully cleaned set to file for fasttext to use\n",
    "    if setnames[i] == 'tune':\n",
    "        with open(f\"{reddit_path}/fasttext_{setnames[i]}.valid\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.writelines(cleaned_lines)\n",
    "    else:\n",
    "        with open(f\"{reddit_path}/fasttext_{setnames[i]}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.writelines(cleaned_lines)\n",
    "    \n",
    "    i += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
